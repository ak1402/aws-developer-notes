-> AWS CLI
	-> to programatically configure aws resources through command prompt

-> AWS CLI Commands
	
	-> EC2
	
		-> Create key value pair
			-> aws ec2 create-key-pair --key-name MyKeyFirstCliPair --output text > MyFirstCliKeyPair.pem
		-> Describe key value pair
			-> aws ec2 describe-key-pairs
		-> Delete key value pair
			-> aws ec2 delete-key-pair --key-name MyKeyFirstCliPair
		-> Create ec2 instance
			-> aws ec2 run-instances --image-id ami-0667e6e9c2c1b6e50 --count 1 --instance-type t2.micro --key-name MyTestCLI --security-group-ids sg-46a59f3d
		-> Terminate ec2 instance
			-> aws ec2 terminate-instances --instance-ids

		-> Creating tags on EC2 instances
			-> aws ec2 create-tags --resource <image-id> --tags Key=<key>, Value=<value>
	
	-> S3
	
		-> Create S3 bucket
			-> aws s3 mb s3://<bucket-name>
		-> Sync directory : Syncs the s3 bucket with local directory
			-> aws s3 sync <folder> s3://<bucket-name>
			-> aws s3 sync <folder> s3://<bucket-name> --delete : Adding delete will sync the deleted files with s3 bucket
		-> Move file from local to s3 or vice versa
			-> aws s3 mv <path> s3://<bucket-name>
			-> aws s3 mv s3://<bucket-name> <folder>
		-> Copy files
			-> aws s3 cp <path> s3://<bucket-name> : Copy file from local to s3
			-> aws s3 cp s3://<bucket-name> <path> : Copy file from s3 to local
			-> aws s3 cp s3://<bucket-name-1> s3://<bucket-name-2> : Copy file from s3 to s3
		-> Delete file
			-> aws s3 rm s3://<bucket-name> --recursive  : delete all objects
			-> aws s3 rm s3://<bucket-name>/<filename> : delete file
			-> aws s3 rm s3://<bucket-name> --recursive --exclude "*<.extension>" : delete all objects except extension
		-> Delete bucket
			-> aws s3 rb s3://<bucket-name>
			-> aws s3 rb s3://<bucket-name> --force : deletes bucket regardles of empty
	
	-> AWS SQS
		
		-> There are two types of SQS
			-> Standard queue
			-> FIFO queue
		-> Standard queue
			-> allows duplicate message id and tries its best to order the message availaable in the queu. however there is a chances
				of duplicate message id processing if the message stored contenguiously in server fails
			-> allow at least 1 processing.
			
		-> FIFO queue
			-> doesnt allow multiple message id in deduplication interval to be available in queue
			-> allow one processing only
			-> mantains the order of the message.
			-> if multiple group id is used, then the order will be on the group id level.
			-> allows 3000 transaction and 300 api calls at onces
			-> suitable for system were order and uniquness of message processing is important
		
		-> Resources for SQS
			-> ApproximateNumberOfMessages
				-> get the appropiate number of messages in the queue
			-> ApproximateNumberOfMessages
				-> get the appropiate number of delayed message
			-> ApproximateNumberOfMessagesNotVisible
				-> get the appropiate number of not visible messages
				
		-> Short Polling
			-> queries subset of servers not entirely.
			-> returns subset of messages.
			-> response is sent right away even if there is no messages
		
		-> Long Polling
			-> Long polling is when RecieveMessage api is greater than 0.
		
		-> Dead letter queue
			-> When messages arent delvered ater multiple tries are placed under dead letter queue.
			-> used for debugging purpose.
			
		-> Visiblity queue
			-> When message are processed they have a fixed visiblity period under which no other consumer can access the message.
		
		-> Delayed queues
			-> Send a message after certain delay period to the queue.
			
	-> AWS SNS
	
		-> publisher and subscriber based approach.

		-> difference btw SNS and SQS is
			-> SNS publish message to a topic which is subscribe by various service (reciever)

		-> max 256kb is allowed, otherwise you can use extended client library that can send 2 gb.
			-> it stores the payload in s3 and shares the reference, the consumer recieve the reference and get the s3 object.

		-> SNS FIFO
			-> strictly manages the order of the message.

		-> Deduplication id
			-> If message is send with a deduplication id , another message cant be accepted but not deleivered during deduplication period.

		-> Message grouping
			-> it is to group the message in a particular groupid.
			-> the order is maintained in the groups.
			
		-> SQS, lambda , https client system are the one that are consumer of SNS.
		
		-> Message filtering is used to filter the messages of which message to be delivered to which consumer.
		
	-> AWS EC2
		
		-> available in compute service in aws -> elastic cloud computing.
		
		-> ec2 instance is generated using AMI Amazon Machine Identity.
		
		-> an ami is the template where you can configure software, OS etc of an ec2 instance.
		
		-> ec2 has root storage which is called EBS elastic block storage.
		
		-> terminating ec2 terminates root volume.
		
		-> security group is per region.
			-> inbound and outbound policy is determined in security group.
		
		-> AMI created can be imported to different region to create ec2 instance
		
		-> you can monitor ec2 using monitoring tools
		
		-> Cloud watch is a monitioring tool where you can send the metric of your ec2 instances.
			-> 5 min is default time, 1 min is paid.
			-> define cloud watch alarm to take actiion on the events that occured in ec2.
		
		-> AWS has been deployed in multiple region and each region is having multiple AZ
			-> AZ are avaialblity zone and there are minimum 3 AZ per region.
			
		-> To add security to your ec2 instance, configure your security group to limit access to you instance
		
		-> you can use private subnet for your instance to stop it getting access from internet.
		-> you can deploy NAT gateway in you instance to access public netwrok.
		
		-> A virtual private cloud (VPC) is a virtual network in your own logically isolated area in the AWS Cloud.
			-> Use seperate VPC to isolate your resouces based on workload or oganisation entity.
			
		-> Call EC2 from a VPC with sending traffic to internet, aws private link.
		
		-> hypervisor isolates CPU and memory.
		
	-> S3
	
		-> Object/File storage in aws cloud.
		
		-> Simple Storage service, can store as much data, required for big data and machine nearling.
		
		-> Create delete synce move upload etc are the some function that you can perform on amazon s3 bucket.
		
		-> To access the object in s3 bucket, you can use the object url(https) from your browser.
			-> you can define bucket policy to manage the access on s3 buckets.
			-> bucket policy can be generate through policy generator
			
		-> you can use Private link to access the s3 bucket without transfering the traffic on internet.
			-> Interface endpoint is an extension to gateway endpoint which uses private ip to connect to aws resource within you VPC or with another VPC.
			-> with the help of private link, you can connect to your s3 bucket over direct connect or VPN.
		
		-> Encryption
			-> There are two types of encyrtion in S3
				-> Server Side
					-> Server side encryption as name suggested is done on the server side. There are two default server side encryption option.
					-> SSE - S3
						-> uses master key to encrypt and decrypt the object stored in s3
						-> CLI command to put object with SSE S3
							-> aws s3 cp <path> <bucket-name> --sse
					
					-> SSE - S3 KMS encryption
						-> while uploading objects, S3 makes a call to KMS service to get the key (1 in plain format and another in cipher format). S3 uses plain format to encrpty the data and stores the object with cipher format key as metadata.
							in decrypting, s3 makes a call again to KMS service passing the encrpted data key and askes to generate a plain text data key. The S3 then decypts the object using plain text data key and sends the request abck to the users.
						
						-> CLI command to put object with SSE S3 KMS
							
							-> aws s3 cp <path> <bucket-name> --sse aws:kms --sse-kms-key-id <key id stored in KMS>
				
				-> Client Side
				
					-> Using Customer managed Key stored in AWS KMS
					
						-> Client sends the CMKID get the symmetric data key from the KMS.						
						-> This data key is used to encrypt the object.						
						-> AWS KMS returns two keys, 1st one is plaintext and second is ciphor blob
						-> The client encrypts the object and attach the cipher key along with it as a meta data.
						-> when downloaded the object, it reciepher the cipher blob key. This is send by the client again to KMS and a plain text output is generated for the same
							to decrypt the object.
					
					-> Using master key stored in your application.
						
						-> provide client master key to amazon encryption client. the client then uses the master key only to encrypt the data key provided by encrption client.
						-> Once the data key is recieved, it is used to encrpty the object and the master key is used to encrpty the datakey as well.
						-> the object is stored to s3 bucket along with the encrypted key and its material description.
						-> when downloading, the client uses the material description to find out which master key is required to depcrypt the key.
						-> once the key is decrypted, it is used to decrpt the object as well.
						
	-> API Gateway
	
		-> Api gateway is the frount gate for the api. It is basically the entry point of api. All the routes security principle log can be attain from here.
		
		-> Api gateway supports 
			-> Http endpoints
			-> Rest endpoints
			-> Websockets.
		
		-> Api Gateway intergrates client with the backend aws services like lambda, http, SNS, SQS, EC2 etc.		
		-> Api Gateway also supports private integration that client can access private resource inside VPC without exposing it to public network.		
		-> Enable cache encryption extend the security of api gateway.
		
	-> Elastic Cache

		-> it is an in memory data storage.
		-> used to cached the data from data staore in order to achieve low latency and high resoponse.
		-> use case would be session storage in which when user logs into application for the first time, it stores the data in elastic cache so that next time 
			he logs in again the value is returned from the cache.
		-> There are two primary in memory database
			-> Redis 
				-> with redis you can keep your data on disk with a point in snapshot which can be used for archival purpose.
				-> redis lets you create multiple replica of primary store, in order to sclae up for read operation.
				-> supports transaction.
				-> redis supports pub sub pattern
				-> no multithreading is supported
				
			-> Memcache
				-> No snapshot
				-> No replica
				-> Doesnt support transaction.
				-> No pub sub pattern
				-> memchached is multithreaded.
		
		-> Security
			-> In transit security (TLS)
				-> enable in transit security on the replica.
			
			-> At rest encryption in elstic cache for redis.
				-> elastic cache for redis at rest encryption using AWS KMS
			-> Redis Auth to get access to redis store.
			-> memchache uses sasl based authentication.
			
		